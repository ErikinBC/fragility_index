# -*- coding: utf-8 -*-
"""BBD_PHN_Neg_FI_script.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/124Bwiogyv1j4UfuJvzRkSV2UymN9BZUS

# Fragility index for the bowel and bladder dysfunction (BBD) and hydronephrosis (PHN) project
"""

# Load modules
import os
import pandas as pd
import numpy as np


#####################################################
# ----- Step 1: Load Support Functions and Data --- #
#####################################################

# Set directory
dir_base = os.getcwd() #'C:\\Users\\erikinwest\\Documents\\SickKids\\projects\\FragilityIndex'
os.chdir(dir_base)
fn_sheets = pd.Series(os.listdir())
fn_sheets = fn_sheets[fn_sheets.str.contains('xlsx')].to_list()

# Load support functions
import support_fi as sf

# Check that sheets exist
fn_all = ['BBD FI', 'Hydro FQ project', 'FI for non-significant studies']
sf.stopifnot(all([x.split('.')[0] in fn_all for x in fn_sheets]))

# ------------------------------------------- #
# --- (i) BLADDER AND BOWEL DISEASE (BBD) --- # 

df_bbd = pd.read_excel(fn_all[0]+'.xlsx')
df_bbd.rename(columns=sf.di_cn_bbd,inplace=True)
sf.stopifnot(~df_bbd[['Group 1','Group 2']].isnull().any(axis=1).any())
# BBD specific mapping
#df_bbd['journal'] = [sf.di_journal_bbd[x] for x in df_bbd.journal]
#df_bbd['geo'] = [sf.di_geo_bbd[x] for x in df_bbd.geo]
df_bbd['study_design'] = [sf.di_design_bbd[x] for x in df_bbd.study_design]
df_bbd['type_random'] = [sf.di_random_bbd[x] for x in df_bbd.type_random]
df_bbd['topic'] = [sf.di_topic_bbd[x] for x in df_bbd.topic]

# Remove non-inferential columns
df_bbd.drop(columns=np.intersect1d(df_bbd.columns, sf.cn_drop),inplace=True)

# Extract the columns needed to calculate FI
cn_FI_bbd = ['ID'] + df_bbd.columns[df_bbd.columns.str.contains('Group\\s[1-2]$')].to_list()
df_bbd_FI = df_bbd[cn_FI_bbd].copy().astype(int)
df_bbd_FI.columns = df_bbd_FI.columns.str.replace('# pts with outcome Group ','num_out').str.replace('Group\\s','num')
df_bbd_FI.insert(0,'tt','bbd')
df_bbd.drop(columns=df_bbd.columns[df_bbd.columns.str.contains('Group')],inplace=True)
df_bbd.insert(0,'tt','bbd')

# --------------------------- #
# --- (ii) HYDRONEPHROSIS --- # 

df_phn = pd.read_excel(fn_all[1]+'.xlsx')
df_phn.rename(columns=sf.di_cn_phn,inplace=True)
sf.stopifnot(~df_phn[['Group 1','Group 2']].isnull().any(axis=1).any())

# Study 612 needs to have columns swapped
df_phn.loc[df_phn.ID == 612,'Group 1'] = \
    df_phn.loc[df_phn.ID == 612,'Group 1 Intervention'].to_list()[0]
df_phn.loc[df_phn.ID == 612,'Group 2'] = \
    df_phn.loc[df_phn.ID == 612,'Group 2 Intervention'].to_list()[0]
# Study 959 has unknown outcome number in group2
# Study 1166 has > 3 million patients
df_phn = df_phn[~df_phn.ID.isin([959, 1166])].reset_index(drop=True)

# PHN specific mapping
#df_phn['journal'] = [sf.di_journal_phn[x] for x in df_phn.journal]
#df_phn['geo'] = [sf.di_geo_phn[x] for x in df_phn.geo]
df_phn['study_design'] = [sf.di_design_phn[x] for x in df_phn.study_design]
df_phn['type_random'] = [sf.di_random_phn[x] for x in df_phn.type_random]
df_phn['topic'] = [sf.di_topic_phn[x] for x in df_phn.topic]

# Remove non-inferential columns
df_phn.drop(columns=np.intersect1d(df_phn.columns, sf.cn_drop),inplace=True)

# Extract the columns needed to calculate FI
cn_FI_phn = ['ID'] + df_phn.columns[df_phn.columns.str.contains('Group\\s[1-2]$')].to_list()
df_phn_FI = df_phn[cn_FI_phn].copy().astype(int)
df_phn_FI.columns = df_phn_FI.columns.str.replace('# pts with outcome Group ','num_out').str.replace('Group\\s','num')
df_phn_FI.insert(0,'tt','phn')
# Remove from original dataframe
df_phn.drop(columns=df_phn.columns[df_phn.columns.str.lower().str.contains('group|grp')],inplace=True)
df_phn.insert(0,'tt','phn')

# -------------------------------- #
# --- (iii) REVERSE FI STUDIES --- # 

df_neg = pd.read_excel(fn_all[2]+'.xlsx')
df_neg.rename(columns=sf.di_cn_neg,inplace=True)
sf.stopifnot(~df_neg[['Group 1','Group 2']].isnull().any(axis=1).any())
# Replace missing IDs
n_ID_neg_miss = df_neg.ID.isnull().sum()
ID_neg_replace = np.arange(999,999-n_ID_neg_miss,-1)
sf.stopifnot(len(np.intersect1d(ID_neg_replace,df_neg.ID))==0)
df_neg.loc[df_neg.ID.isnull(),'ID'] = ID_neg_replace
df_neg['ID'] = df_neg.ID.astype(int)

# Negative specific mapping
#df_neg['journal'] = [sf.di_journal_neg[x] for x in df_neg.journal]
#df_neg['geo'] = [sf.di_geo_neg[x] for x in df_neg.geo]
df_neg['study_design'] = [sf.di_design_neg[x] for x in df_neg.study_design]
df_neg['type_random'] = [sf.di_random_neg[x] for x in df_neg.type_random]
df_neg['topic'] = [sf.di_topic_neg[x] for x in df_neg.topic]


# Remove non-inferential columns
df_neg.drop(columns=np.intersect1d(df_neg.columns, sf.cn_drop),inplace=True)

# Extract the columns needed to calculate FI
cn_FI_neg = ['ID'] + df_neg.columns[df_neg.columns.str.contains('Group\\s[1-2]$')].to_list()
df_neg_FI = df_neg[cn_FI_neg].copy()
df_neg_FI.columns = df_neg_FI.columns.str.replace('# pts with outcome Group ','num_out').str.replace('Group\\s','num')
df_neg_FI.insert(0,'tt','neg')
df_neg.drop(columns=df_neg.columns[df_neg.columns.str.contains('Group')],inplace=True)
df_neg.insert(0,'tt','neg')

#########################################
# ----- Step 2: Merge Data and Save --- #
#########################################

# Merge the binary outcomes for fragility index counts
df_FI = pd.concat([df_bbd_FI, df_phn_FI, df_neg_FI]).reset_index(drop=True)
# Merge the inferential columns
df_inf = pd.concat([df_bbd, df_phn, df_neg],sort=False).reset_index(drop=True)
# Apply consistent mapping
df_inf['blinding'] = [sf.di_blinding[x] for x in df_inf.blinding]
df_inf['concealment'] = [sf.di_conceal[x] for x in df_inf.concealment]
df_inf['funding'] = [sf.di_funding[x] for x in df_inf.funding]
df_inf['outcome'] = [sf.di_outcome[x] for x in df_inf.outcome]
df_inf['power_just'] = df_inf.power_just.fillna(0) # replace NA with 0 (none)
df_inf['power_just'] = [sf.di_power[x] for x in df_inf.power_just]
df_inf['statistician'] = [sf.di_statistician[x] for x in df_inf.statistician]
# Do feature transformations for consistent comparisons
df_inf['study_design'] = np.where(df_inf.study_design.str.contains('RCT'),'RCT','Other')

# --- SAVE --- #
df_FI.to_csv(os.path.join(dir_base,'df_FI.csv'),index=False)
df_inf.to_csv(os.path.join(dir_base,'df_inf.csv'),index=False)

#def whatisdiff(x,y):
#    d1 = ', '.join(np.setdiff1d(x,y).astype(str))
#    d2 = ', '.join(np.setdiff1d(y,x).astype(str))
#    print('In x but not y: %s\nIn y but not x: %s' % (d1, d2))
#whatisdiff(df_bbd.columns, df_phn.columns)
#whatisdiff(df_bbd.columns, df_neg.columns)
#whatisdiff(df_neg.columns, df_phn.columns)
#
#di_inf = {'bbd':df_bbd, 'phn':df_phn, 'neg':df_neg}
#
#tmp = []
#for tt in di_inf.keys():
#    tmp.append(pd.DataFrame({'tt':tt,'cn':di_inf[tt].columns,'dt':di_inf[tt].dtypes}))
#df_cn = pd.concat(tmp).reset_index(drop=True)
#df_cn['dt'] = df_cn.dt.astype(str).str.replace('[0-9]','')
#df_cn = df_cn.groupby(['cn','dt']).size().reset_index().pivot('cn','dt',0).reset_index().fillna(0)
#print(df_cn[~df_cn.iloc[:,1:].apply(lambda x: len(x[x > 0])==1,axis=1)])
